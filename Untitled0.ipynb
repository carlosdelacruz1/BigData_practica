{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVA6xIZXPdNKh/5m51KvEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosdelacruz1/BigData_practica/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKOaOwRiI8d8"
      },
      "outputs": [],
      "source": [
        "### Scraper para Nature extraccion datos web\n",
        "# primero instalamos las herramientas necesarias\n",
        "\n",
        "##El scraper procesa los datos de la web, buscando patrones como \"redox flow batteries\" y de dichos articulos extrae de los abstract la informacion relevante y que almacenaremos\n",
        "\n",
        "!pip install scrapy\n",
        "\n",
        "import scrapy\n",
        "import json\n",
        "\n",
        "class SpiderNature(scrapy.Spider):\n",
        "    name = 'spidernature'\n",
        "    # Podeis cambiar la url inicial por otra u otras paginas\n",
        "    start_urls = ['https://www.nature.com/?utm_term=101248&utm_source=awin&utm_medium=affiliate&awc=26427_1665247653_725c1a43c037016f477ab25153739456']\n",
        "    \n",
        "    # con esto limito no hacer mas de 10 ejecuciones \n",
        "    COUNT_MAX = 1\n",
        "    count = 0\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Aqui scrapeamos los datos y los imprimimos a un fichero\n",
        "        for article in response.css('div.post-item'):\n",
        "            title_text = article.css('div.post-header h2 a ::text').extract_first().strip().replace(',', '').replace('.', '')\n",
        "            extract_text = article.css('div.post-content p ::text').extract_first()\n",
        "            if extract_text is not None:\n",
        "              extract_text = extract_text.strip().replace(',', '').replace('.', '')\n",
        "              \n",
        "            # Print a un fichero\n",
        "            print(f\"{title_text} {extract_text}\\n\")#, file=filep)\n",
        "\n",
        "        # Aqui hacemos crawling (con el follow)\n",
        "        for next_page in response.css('a.next-posts-link'):\n",
        "            self.count = self.count + 1\n",
        "            if (self.count < self.COUNT_MAX):\n",
        "                yield response.follow(next_page, self.parse)\n",
        "\n",
        "      # Podeis cambiar la extension y el nombre del fichero data.txt\n",
        "#filep = open('/content/drive/My Drive/webcrawling/data.csv', 'w')\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(BlogSpider)\n",
        "process.start()\n",
        "filep.close()\n",
        "\n",
        "##cambiar nombre de archivos\n",
        "from google.colab import files\n",
        "files.download('/content/drive/My Drive/webcrawling/data.csv')\n",
        "\n",
        "\n",
        "### SUBIR archivos al buckett\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####\n",
        "\n",
        "\n",
        "##Finalmente podemos proceder a cargar datos de ejemplo dentro de elasticsearch:\n",
        "##Una vez que contamos con un server de elasticsearch podemos cargar datos de prueba para el siguiente ejercicio utilizando Python y el API REST de elasticsearch:\n",
        "\n",
        "# Authenticate\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Setup values\n",
        "\n",
        "project_id = 'bigdata-practica' \n",
        "bucket_name = 'bucket-elastic'\n",
        "\n",
        "# Import libs\n",
        "from googleapiclient.discovery import build\n",
        "gcs_service = build('storage', 'v1')\n",
        "\n",
        "## importar datos al bucket con elastic\n",
        "\n",
        "# Let's use elasticsearch python package for simplicity\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch(\n",
        "    server_ip\n",
        ")\n",
        "\n",
        "with open(local_file, 'r') as f:\n",
        "  data = f.read()\n",
        "  # Send the data into es\n",
        "  es.bulk(operations=data, index='shakespeare')\n",
        "\n",
        "  requests.get('{}/shakespeare/{}'.format(server_ip, '_search?pretty=true&q=*:*')).json()"
      ]
    }
  ]
}